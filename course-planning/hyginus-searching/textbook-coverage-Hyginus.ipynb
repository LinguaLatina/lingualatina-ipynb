{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook coverage of vocabulary in Hyginus, *Fabulae*\n",
    "\n",
    "Read vocabulary lists for L3 textbook, compute percentage of total tokens in Hyginus (excluding proper names) covered by accumulated vocabulary for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: display coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing coverage of unit 1 ... 29.3%\n",
      "Computing coverage of unit 2 ... 40.8%\n",
      "Computing coverage of unit 3 ... 58.2%\n",
      "Computing coverage of unit 4 ... 62.9%\n",
      "Computing coverage of unit 5 ... 75.2%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>L3 vocabulary lists: coverage of Hyginus</h2><table><tr><th>Unit</th><th>Pct. Hyginus</th><th>Vocabulary items</th></tr><tr><td>Vocabulary through unit 1 vocabulary</td><td>covers <strong>29.3%</strong> of tokens in Hyginus</td><td><strong>35</strong> vocabulary items</td></tr>\n",
       "<tr><td>Vocabulary through unit 2 vocabulary</td><td>covers <strong>40.8%</strong> of tokens in Hyginus</td><td><strong>69</strong> vocabulary items</td></tr>\n",
       "<tr><td>Vocabulary through unit 3 vocabulary</td><td>covers <strong>58.2%</strong> of tokens in Hyginus</td><td><strong>109</strong> vocabulary items</td></tr>\n",
       "<tr><td>Vocabulary through unit 4 vocabulary</td><td>covers <strong>62.9%</strong> of tokens in Hyginus</td><td><strong>144</strong> vocabulary items</td></tr>\n",
       "<tr><td>Vocabulary through unit 5 vocabulary</td><td>covers <strong>75.2%</strong> of tokens in Hyginus</td><td><strong>168</strong> vocabulary items</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: load everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvocabFiles\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m5\u001b[39m -> \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/05-questions.cex\"\u001b[39m,\n",
       "  \u001b[32m1\u001b[39m -> \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/01-nouns-adjs-pron.cex\"\u001b[39m,\n",
       "  \u001b[32m2\u001b[39m -> \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/02-verbs.cex\"\u001b[39m,\n",
       "  \u001b[32m3\u001b[39m -> \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/03-place-and-time.cex\"\u001b[39m,\n",
       "  \u001b[32m4\u001b[39m -> \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/04-verbal-nouns-and-adjectives.cex\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Survey all vocab files in this list:\n",
    "val vocabFiles : Map[Int, String] = Map(\n",
    "  1 -> \"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/01-nouns-adjs-pron.cex\",\n",
    "  2 -> \"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/02-verbs.cex\",\n",
    "  3 -> \"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/03-place-and-time.cex\",\n",
    "  4 -> \"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/04-verbal-nouns-and-adjectives.cex\",\n",
    "  5 -> \"https://raw.githubusercontent.com/LinguaLatina/textbook/master/vocablists/05-questions.cex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpersonalRepo\u001b[39m: \u001b[32mcoursierapi\u001b[39m.\u001b[32mMavenRepository\u001b[39m = MavenRepository(https://dl.bintray.com/neelsmith/maven)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// set up notebook to find repository\n",
    "val personalRepo = coursierapi.MavenRepository.of(\"https://dl.bintray.com/neelsmith/maven\")\n",
    "interp.repositories() ++= Seq(personalRepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                          \u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ivy imports\n",
    "import $ivy.`edu.holycross.shot::latincorpus:7.0.0-pr5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36medu.holycross.shot.latincorpus._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import edu.holycross.shot.latincorpus._\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mhyginusUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://raw.githubusercontent.com/LinguaLatina/analysis/master/data/hyginus/hyginus-latc.cex\"\u001b[39m\n",
       "\u001b[36mhyginus\u001b[39m: \u001b[32mLatinCorpus\u001b[39m = \u001b[33mLatinCorpus\u001b[39m(\n",
       "  \u001b[33mVector\u001b[39m(\n",
       "    \u001b[33mLatinParsedToken\u001b[39m(\n",
       "      \u001b[33mCitableNode\u001b[39m(\n",
       "        \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:t.1.0\"\u001b[39m),\n",
       "        \u001b[32m\"EXCERPTA\"\u001b[39m\n",
       "      ),\n",
       "      LexicalToken,\n",
       "      \u001b[33mVector\u001b[39m()\n",
       "    ),\n",
       "    \u001b[33mLatinParsedToken\u001b[39m(\n",
       "      \u001b[33mCitableNode\u001b[39m(\n",
       "        \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:t.1.1\"\u001b[39m),\n",
       "        \u001b[32m\"EX\"\u001b[39m\n",
       "      ),\n",
       "      LexicalToken,\n",
       "      \u001b[33mVector\u001b[39m(\u001b[33mIndeclinableForm\u001b[39m(\u001b[32m\"ls.n16519\"\u001b[39m, \u001b[32m\"\"\u001b[39m, \u001b[32m\"\"\u001b[39m, Conjunction))\n",
       "    ),\n",
       "    \u001b[33mLatinParsedToken\u001b[39m(\n",
       "      \u001b[33mCitableNode\u001b[39m(\n",
       "        \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:t.1.2\"\u001b[39m),\n",
       "        \u001b[32m\"HYGINI\"\u001b[39m\n",
       "      ),\n",
       "      LexicalToken,\n",
       "      \u001b[33mVector\u001b[39m()\n",
       "    ),\n",
       "    \u001b[33mLatinParsedToken\u001b[39m(\n",
       "      \u001b[33mCitableNode\u001b[39m(\n",
       "        \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:t.1.3\"\u001b[39m),\n",
       "        \u001b[32m\"GENEALOGIIS\"\u001b[39m\n",
       "      ),\n",
       "      LexicalToken,\n",
       "      \u001b[33mVector\u001b[39m()\n",
       "    ),\n",
       "    \u001b[33mLatinParsedToken\u001b[39m(\n",
       "      \u001b[33mCitableNode\u001b[39m(\n",
       "        \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:t.1.3_0\"\u001b[39m),\n",
       "        \u001b[32m\",\"\u001b[39m\n",
       "      ),\n",
       "..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hyginusUrl = \"https://raw.githubusercontent.com/LinguaLatina/analysis/master/data/hyginus/hyginus-latc.cex\"\n",
    "val hyginus = LatinCorpus.fromUrl(hyginusUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokens\u001b[39m: \u001b[32mVector\u001b[39m[\u001b[32mLatinParsedToken\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[33mLatinParsedToken\u001b[39m(\n",
       "    \u001b[33mCitableNode\u001b[39m(\n",
       "      \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:pr.1.3\"\u001b[39m),\n",
       "      \u001b[32m\"ex\"\u001b[39m\n",
       "    ),\n",
       "    LexicalToken,\n",
       "    \u001b[33mVector\u001b[39m(\u001b[33mIndeclinableForm\u001b[39m(\u001b[32m\"ls.n16519\"\u001b[39m, \u001b[32m\"\"\u001b[39m, \u001b[32m\"\"\u001b[39m, Conjunction))\n",
       "  ),\n",
       "  \u001b[33mLatinParsedToken\u001b[39m(\n",
       "    \u001b[33mCitableNode\u001b[39m(\n",
       "      \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:pr.1.5\"\u001b[39m),\n",
       "      \u001b[32m\"et\"\u001b[39m\n",
       "    ),\n",
       "    LexicalToken,\n",
       "    \u001b[33mVector\u001b[39m(\u001b[33mIndeclinableForm\u001b[39m(\u001b[32m\"ls.n16278\"\u001b[39m, \u001b[32m\"\"\u001b[39m, \u001b[32m\"\"\u001b[39m, Conjunction))\n",
       "  ),\n",
       "  \u001b[33mLatinParsedToken\u001b[39m(\n",
       "    \u001b[33mCitableNode\u001b[39m(\n",
       "      \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:pr.1.11\"\u001b[39m),\n",
       "      \u001b[32m\"ex\"\u001b[39m\n",
       "    ),\n",
       "    LexicalToken,\n",
       "    \u001b[33mVector\u001b[39m(\u001b[33mIndeclinableForm\u001b[39m(\u001b[32m\"ls.n16519\"\u001b[39m, \u001b[32m\"\"\u001b[39m, \u001b[32m\"\"\u001b[39m, Conjunction))\n",
       "  ),\n",
       "  \u001b[33mLatinParsedToken\u001b[39m(\n",
       "    \u001b[33mCitableNode\u001b[39m(\n",
       "      \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:pr.1.13\"\u001b[39m),\n",
       "      \u001b[32m\"et\"\u001b[39m\n",
       "    ),\n",
       "    LexicalToken,\n",
       "    \u001b[33mVector\u001b[39m(\u001b[33mIndeclinableForm\u001b[39m(\u001b[32m\"ls.n16278\"\u001b[39m, \u001b[32m\"\"\u001b[39m, \u001b[32m\"\"\u001b[39m, Conjunction))\n",
       "  ),\n",
       "  \u001b[33mLatinParsedToken\u001b[39m(\n",
       "    \u001b[33mCitableNode\u001b[39m(\n",
       "      \u001b[33mCtsUrn\u001b[39m(\u001b[32m\"urn:cts:latinLit:stoa1263.stoa001.hc_tkns:pr.1.23\"\u001b[39m),\n",
       "      \u001b[32m\"id\"\u001b[39m\n",
       "    ),\n",
       "    LexicalToken,\n",
       "...\n",
       "\u001b[36mtotal\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m20913\u001b[39m\n",
       "\u001b[36mtotalAnalyzed\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m18605\u001b[39m\n",
       "\u001b[36manalysisCoverage\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m88.96380241954766\u001b[39m\n",
       "\u001b[36manalysisPct\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m89.0\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokens = hyginus.tokens.filter(_.text.head.isLower)\n",
    "\n",
    "val total = tokens.size\n",
    "val totalAnalyzed = tokens.filter(_.analyses.nonEmpty).size\n",
    "\n",
    "val analysisCoverage = (totalAnalyzed * 1.0 / total) * 100\n",
    "val analysisPct = BigDecimal(analysisCoverage).setScale(1, BigDecimal.RoundingMode.HALF_UP).toDouble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtempOmit\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"ls.n49983\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n40071\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n25107\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n28700\"\u001b[39m,\n",
       "  \u001b[32m\"ls.38383\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n40913\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n30584\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n31181\"\u001b[39m,\n",
       "  \u001b[32m\"ls.n31181\"\u001b[39m\n",
       ")\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mvocabForUnit\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36munitCoverage\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Omit these IDs until parser recompiled\n",
    "val tempOmit = List(\n",
    "  \"ls.n49983\", \n",
    "  \"ls.n40071\",\n",
    "  \"ls.n25107\", \n",
    "  \"ls.n28700\",\n",
    "  \"ls.38383\", \n",
    "  \"ls.n40913\",\n",
    "  \"ls.n30584\", // -ne\n",
    "  \"ls.n31181\", // nonne\n",
    "  \"ls.n31181\" // num\n",
    ")\n",
    "\n",
    "def vocabForUnit(vocabUnit: Int): Vector[String] = {\n",
    "  val vocab = for (i <- 1 to vocabUnit) yield {\n",
    "    val lines = Source.fromURL(vocabFiles(i))\n",
    "    val lexemeIds = lines.getLines.toVector.tail.filter(_.nonEmpty).map( ln => {\n",
    "      val columns = ln.split(\"#\")\n",
    "      val idParts = columns.head.split(\":\")\n",
    "      idParts.head\n",
    "    })\n",
    "    lexemeIds\n",
    "  }\n",
    "  vocab.toVector.flatten.filterNot(v => tempOmit.contains(v))\n",
    "}\n",
    "\n",
    "def unitCoverage(vocabUnit: Int) = {\n",
    "  val counts = vocabForUnit(vocabUnit).map(lex => hyginus.passagesForLexeme(lex).size)\n",
    "  val unitCoverage = (counts.sum * 1.0 / total) * 100\n",
    "  val unitPct = BigDecimal(unitCoverage).setScale(1, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "  (unitPct, counts.size)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msurvey\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def survey = {\n",
    "  val rows = for (i <- 1 to vocabFiles.size) yield {\n",
    "    print(\"Computing coverage of unit \" + i + \" ... \")\n",
    "    val (pct, vocabSize) = unitCoverage(i)\n",
    "    println(pct + \"%\")\n",
    "    \"<tr>\"+\n",
    "    s\"<td>Vocabulary through unit ${i} vocabulary</td>\" +\n",
    "    s\"<td>covers <strong>${pct}%</strong> of tokens in Hyginus</td>\" +\n",
    "    s\"<td><strong>${vocabSize}</strong> vocabulary items</td>\" +\n",
    "    \"</tr>\"\n",
    "  }\n",
    "  \n",
    "  val header = \"<h2>L3 vocabulary lists: coverage of Hyginus</h2>\"\n",
    "  val tableHeader = \"<tr><th>Unit</th><th>Pct. Hyginus</th><th>Vocabulary items</th></tr>\"\n",
    "  Html(header + \"<table>\"  + tableHeader + rows.mkString(\"\\n\") + \"</table>\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "nteract": {
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
